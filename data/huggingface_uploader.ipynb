{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "55df829f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Dict, List, Any, Optional, Tuple\n",
    "import os\n",
    "import json\n",
    "import glob\n",
    "import random\n",
    "from datasets import Dataset, DatasetDict, Features, Value, Image as HFImage, ClassLabel, Sequence\n",
    "from dotenv import load_dotenv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "cef2228d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "  \"task_id\": \"_flip_bafkreia2ftormk5ydiy4rb4wamzvuxr5g6bzmzu3lxdzxs4rncwsmcakne\",\n",
      "  \"images\": {\n",
      "    \"0\": \"b3e5851e-2eb3-4a23-bd1a-c111311ebcd1\",\n",
      "    \"1\": \"582e5ff0-ca73-498e-aecf-2d6ea6e6e8bd\",\n",
      "    \"2\": \"1c642f13-e1d5-44a1-ba6d-152a80121328\",\n",
      "    \"3\": \"c3ac47de-429d-4cec-8242-139ac6a61bdf\"\n",
      "  },\n",
      "  \"left_stack\": [\n",
      "    \"3\",\n",
      "    \"2\",\n",
      "    \"1\",\n",
      "    \"0\"\n",
      "  ],\n",
      "  \"right_stack\": [\n",
      "    \"3\",\n",
      "    \"0\",\n",
      "    \"1\",\n",
      "    \"2\"\n",
      "  ],\n",
      "  \"agreed_answer\": [\n",
      "    \"Right\",\n",
      "    \"Strong\"\n",
      "  ],\n",
      "  \"votes\": {\n",
      "    \"Left\": \"2\",\n",
      "    \"Right\": \"10\",\n",
      "    \"Reported\": \"0\"\n",
      "  },\n",
      "  \"details\": {\n",
      "    \"Author:\": \"0xC0FF90dE1a01b42345762bF77708F1255cB2d071\",\n",
      "    \"Epoch:\": \"#0011\",\n",
      "    \"Size:\": \"106224 bytes\",\n",
      "    \"Created:\": \"10/9/2019 15:12:47\",\n",
      "    \"Block:\": \"322755\",\n",
      "    \"Tx:\": \"0x3194b518172d61f16878050a5ccd83758037080efe458a92fb71e99c161c284f\"\n",
      "  }\n",
      "}\n",
      "\n",
      "Verification:\n",
      "Task ID: _flip_bafkreia2ftormk5ydiy4rb4wamzvuxr5g6bzmzu3lxdzxs4rncwsmcakne\n",
      "Number of images: 4\n",
      "Left stack: ['3', '2', '1', '0']\n",
      "Right stack: ['3', '0', '1', '2']\n",
      "Left and right stacks contain the same images: True\n"
     ]
    }
   ],
   "source": [
    "def extract_image_id(blob_url: str) -> str:\n",
    "    \"\"\"Extract the image ID from a blob URL.\n",
    "    \n",
    "    Args:\n",
    "        blob_url: The blob URL containing the image ID\n",
    "        \n",
    "    Returns:\n",
    "        The extracted image ID\n",
    "    \"\"\"\n",
    "    # Strip \"blob:https://scan.idena.io/\" from the URL\n",
    "    prefix = \"blob:https://scan.idena.io/\"\n",
    "    if blob_url.startswith(prefix):\n",
    "        return blob_url[len(prefix):]\n",
    "    return blob_url\n",
    "\n",
    "def process_task_data(task_data: dict[str, Any]) -> dict[str, Any]:\n",
    "    \"\"\"Process task data to transform it into the required format.\n",
    "    \n",
    "    Args:\n",
    "        task_data: The original task data\n",
    "        \n",
    "    Returns:\n",
    "        Processed task data in the required format\n",
    "    \"\"\"\n",
    "    # Extract task ID from name field\n",
    "    task_id = task_data.get(\"name\", \"\").replace(\"/\", \"_\")\n",
    "    \n",
    "    # Extract image IDs from image_lst1 and image_lst2\n",
    "    image_ids = set()\n",
    "    for lst in [\"image_lst1\", \"image_lst2\"]:\n",
    "        if lst in task_data and isinstance(task_data[lst], dict):\n",
    "            for _, blob_url in task_data[lst].items():\n",
    "                image_id = extract_image_id(blob_url)\n",
    "                image_ids.add(image_id)\n",
    "    \n",
    "    # Shuffle image IDs and create a mapping\n",
    "    image_ids_list = list(image_ids)\n",
    "    random.shuffle(image_ids_list)\n",
    "    \n",
    "    # Create images dictionary with shuffled order\n",
    "    images_dict = {}\n",
    "    for idx, img_id in enumerate(image_ids_list):\n",
    "        images_dict[str(idx)] = img_id\n",
    "    \n",
    "    # Create inverse mapping for easy lookup\n",
    "    img_id_to_key = {img_id: key for key, img_id in images_dict.items()}\n",
    "    \n",
    "    # Map image lists to their shuffled indices\n",
    "    left_stack = []\n",
    "    right_stack = []\n",
    "    \n",
    "    if \"image_lst1\" in task_data:\n",
    "        for _, blob_url in sorted(task_data[\"image_lst1\"].items(), key=lambda x: int(x[0])):\n",
    "            img_id = extract_image_id(blob_url)\n",
    "            left_stack.append(img_id_to_key[img_id])\n",
    "    \n",
    "    if \"image_lst2\" in task_data:\n",
    "        for _, blob_url in sorted(task_data[\"image_lst2\"].items(), key=lambda x: int(x[0])):\n",
    "            img_id = extract_image_id(blob_url)\n",
    "            right_stack.append(img_id_to_key[img_id])\n",
    "    \n",
    "    # Create new task data structure\n",
    "    processed_data = {\n",
    "        \"task_id\": task_id,\n",
    "        \"images\": images_dict,\n",
    "        \"left_stack\": left_stack,\n",
    "        \"right_stack\": right_stack,\n",
    "        \"agreed_answer\": task_data.get(\"agreed_answer\", []),\n",
    "        \"votes\": task_data.get(\"votes\", {}),\n",
    "        \"details\": task_data.get(\"details\", {})\n",
    "    }\n",
    "    \n",
    "    return processed_data\n",
    "\n",
    "\n",
    "def test_task_processing() -> None:\n",
    "    \"\"\"Test the task processing with a sample task.\"\"\"\n",
    "    # Sample task data\n",
    "    sample_task = {\n",
    "        \"name\": \"/flip/bafkreia2ftormk5ydiy4rb4wamzvuxr5g6bzmzu3lxdzxs4rncwsmcakne\",\n",
    "        \"image_lst1\": {\n",
    "            \"0\": \"blob:https://scan.idena.io/c3ac47de-429d-4cec-8242-139ac6a61bdf\",\n",
    "            \"1\": \"blob:https://scan.idena.io/1c642f13-e1d5-44a1-ba6d-152a80121328\",\n",
    "            \"2\": \"blob:https://scan.idena.io/582e5ff0-ca73-498e-aecf-2d6ea6e6e8bd\",\n",
    "            \"3\": \"blob:https://scan.idena.io/b3e5851e-2eb3-4a23-bd1a-c111311ebcd1\"\n",
    "        },\n",
    "        \"image_lst2\": {\n",
    "            \"0\": \"blob:https://scan.idena.io/c3ac47de-429d-4cec-8242-139ac6a61bdf\",\n",
    "            \"1\": \"blob:https://scan.idena.io/b3e5851e-2eb3-4a23-bd1a-c111311ebcd1\",\n",
    "            \"2\": \"blob:https://scan.idena.io/582e5ff0-ca73-498e-aecf-2d6ea6e6e8bd\",\n",
    "            \"3\": \"blob:https://scan.idena.io/1c642f13-e1d5-44a1-ba6d-152a80121328\"\n",
    "        },\n",
    "        \"agreed_answer\": [\n",
    "            \"Right\",\n",
    "            \"Strong\"\n",
    "        ],\n",
    "        \"votes\": {\n",
    "            \"Left\": \"2\",\n",
    "            \"Right\": \"10\",\n",
    "            \"Reported\": \"0\"\n",
    "        },\n",
    "        \"details\": {\n",
    "            \"Author:\": \"0xC0FF90dE1a01b42345762bF77708F1255cB2d071\",\n",
    "            \"Epoch:\": \"#0011\",\n",
    "            \"Size:\": \"106224 bytes\",\n",
    "            \"Created:\": \"10/9/2019 15:12:47\",\n",
    "            \"Block:\": \"322755\",\n",
    "            \"Tx:\": \"0x3194b518172d61f16878050a5ccd83758037080efe458a92fb71e99c161c284f\"\n",
    "        }\n",
    "    }\n",
    "    \n",
    "    # Set seed for reproducibility\n",
    "    random.seed(42)\n",
    "    \n",
    "    # Process the task\n",
    "    processed_task = process_task_data(sample_task)\n",
    "    \n",
    "    # Print the result\n",
    "    print(json.dumps(processed_task, indent=2))\n",
    "    \n",
    "    # Verify the structure\n",
    "    print(\"\\nVerification:\")\n",
    "    print(f\"Task ID: {processed_task['task_id']}\")\n",
    "    print(f\"Number of images: {len(processed_task['images'])}\")\n",
    "    print(f\"Left stack: {processed_task['left_stack']}\")\n",
    "    print(f\"Right stack: {processed_task['right_stack']}\")\n",
    "    \n",
    "    # Verify that stacks contain the same image indices but in different orders\n",
    "    left_set = set(processed_task['left_stack'])\n",
    "    right_set = set(processed_task['right_stack'])\n",
    "    \n",
    "    print(f\"Left and right stacks contain the same images: {left_set == right_set}\")\n",
    "\n",
    "\n",
    "test_task_processing()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "abef2444",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def read_json_file(file_path: str) -> Dict[str, Any]:\n",
    "    \"\"\"Read a JSON file and return its contents.\n",
    "    \n",
    "    Args:\n",
    "        file_path: Path to the JSON file\n",
    "        \n",
    "    Returns:\n",
    "        The contents of the JSON file as a dictionary\n",
    "    \"\"\"\n",
    "    with open(file_path, 'r') as f:\n",
    "        return json.load(f)\n",
    "\n",
    "def find_image_file(image_id: str, images_dir: str, extensions: Optional[List[str]] = None) -> Optional[str]:\n",
    "    \"\"\"Find the image file for a given image ID.\n",
    "    \n",
    "    Args:\n",
    "        image_id: ID of the image\n",
    "        images_dir: Directory containing the images\n",
    "        extensions: List of image extensions to search for\n",
    "        \n",
    "    Returns:\n",
    "        The path to the image file, or None if not found\n",
    "    \"\"\"\n",
    "    if extensions is None:\n",
    "        # Get extensions from environment or use default\n",
    "        extensions_str = os.getenv(\"IMAGE_EXTENSIONS\", \"png,jpg,jpeg\")\n",
    "        extensions = extensions_str.split(\",\")\n",
    "    \n",
    "    for ext in extensions:\n",
    "        image_path = os.path.join(images_dir, f\"{image_id}.{ext}\")\n",
    "        if os.path.exists(image_path):\n",
    "            return image_path\n",
    "    \n",
    "    return None\n",
    "\n",
    "\n",
    "\n",
    "def load_split_data(split_dir: str) -> Dict[str, List]:\n",
    "    \"\"\"Load data for a single split (train, test, or validation).\n",
    "    \n",
    "    Args:\n",
    "        split_dir: Directory containing the split data\n",
    "        \n",
    "    Returns:\n",
    "        A dictionary with lists for each field in the dataset\n",
    "    \"\"\"\n",
    "    tasks_dir = os.path.join(split_dir, \"tasks\")\n",
    "    images_dir = os.path.join(split_dir, \"images\")\n",
    "    \n",
    "    task_files = glob.glob(os.path.join(tasks_dir, \"*.json\"))\n",
    "    \n",
    "    # These will store the data for each example in the dataset\n",
    "    task_ids = []\n",
    "    task_data_list = []\n",
    "    image_paths = []\n",
    "    image_ids = []\n",
    "    \n",
    "    for task_file in task_files:\n",
    "        try:\n",
    "            task_data = read_json_file(task_file)\n",
    "            \n",
    "            # Process task data\n",
    "            processed_data = process_task_data(task_data)\n",
    "            task_id = processed_data[\"task_id\"]\n",
    "            \n",
    "            # For each image in the task, create a dataset entry\n",
    "            for img_key, img_id in processed_data[\"images\"].items():\n",
    "                img_path = find_image_file(img_id, images_dir)\n",
    "                if img_path:\n",
    "                    task_ids.append(task_id)\n",
    "                    # Store processed data as JSON string\n",
    "                    task_data_list.append(json.dumps(processed_data))\n",
    "                    image_paths.append(img_path)\n",
    "                    image_ids.append(img_id)\n",
    "                else:\n",
    "                    print(f\"Warning: Image file not found for ID: {img_id} in {images_dir}\")\n",
    "        except Exception as e:\n",
    "            print(f\"Error processing task file {task_file}: {e}\")\n",
    "    \n",
    "    return {\n",
    "        \"task_id\": task_ids,\n",
    "        \"task_data\": task_data_list,\n",
    "        \"image_id\": image_ids,\n",
    "        \"image\": image_paths,\n",
    "    }\n",
    "\n",
    "def create_hf_dataset(base_dir: str) -> DatasetDict:\n",
    "    \"\"\"Create a Hugging Face dataset from the directory structure.\n",
    "    \n",
    "    Args:\n",
    "        base_dir: Base directory containing train, test, and validation splits\n",
    "        \n",
    "    Returns:\n",
    "        A DatasetDict object with train, test, and validation splits\n",
    "    \"\"\"\n",
    "    splits = [\"train\", \"test\", \"validation\"]\n",
    "    dataset_dict = {}\n",
    "    \n",
    "    for split in splits:\n",
    "        print(f\"Processing split: {split}\")\n",
    "        split_dir = os.path.join(base_dir, split)\n",
    "        if not os.path.exists(split_dir):\n",
    "            print(f\"Warning: Split directory {split_dir} does not exist. Skipping.\")\n",
    "            continue\n",
    "        \n",
    "        # Load data for this split\n",
    "        split_data = load_split_data(split_dir)\n",
    "        \n",
    "        # Create dataset\n",
    "        features = Features({\n",
    "            \"task_id\": Value(\"string\"),\n",
    "            \"task_data\": Value(\"string\"),\n",
    "            \"image_id\": Value(\"string\"),\n",
    "            \"image\": HFImage(),\n",
    "        })\n",
    "        \n",
    "        dataset = Dataset.from_dict(\n",
    "            split_data,\n",
    "            features=features,\n",
    "        )\n",
    "        \n",
    "        dataset_dict[split] = dataset\n",
    "    \n",
    "    return DatasetDict(dataset_dict)\n",
    "\n",
    "def push_dataset_to_hf(dataset: DatasetDict, dataset_name: str, token: Optional[str] = None) -> None:\n",
    "    \"\"\"Push the dataset to Hugging Face.\n",
    "    \n",
    "    Args:\n",
    "        dataset: The dataset to push\n",
    "        dataset_name: Name of the dataset on Hugging Face\n",
    "        token: Hugging Face API token. If None, will use the token from the Hugging Face CLI.\n",
    "    \"\"\"\n",
    "    # Push to Hugging Face\n",
    "    dataset.push_to_hub(\n",
    "        dataset_name,\n",
    "        token=token,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2d02c0a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "2cb2472d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating dataset from flip_dataset/...\n",
      "Processing split: train\n",
      "Processing split: test\n",
      "Processing split: validation\n"
     ]
    }
   ],
   "source": [
    "load_dotenv(\"../access_tokens.env\")\n",
    "\n",
    "random.seed(42)\n",
    "\n",
    "base_dir=\"flip_dataset/\"\n",
    "dataset_name=\"FLIP-Challenge\"\n",
    "token=os.getenv(\"HF_TOKEN\")\n",
    "\n",
    "print(f\"Creating dataset from {base_dir}...\")\n",
    "dataset = create_hf_dataset(base_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "1020a219",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pushing dataset to Hugging Face as FLIP-Challenge...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 11404/11404 [00:01<00:00, 9155.60 examples/s]\n",
      "Creating parquet from Arrow format: 100%|██████████| 115/115 [00:00<00:00, 174.03ba/s]\n",
      "Map: 100%|██████████| 11403/11403 [00:01<00:00, 8755.01 examples/s].20s/it]\n",
      "Creating parquet from Arrow format: 100%|██████████| 115/115 [00:00<00:00, 180.53ba/s]\n",
      "Map: 100%|██████████| 11403/11403 [00:01<00:00, 8549.14 examples/s].68s/it]\n",
      "Creating parquet from Arrow format: 100%|██████████| 115/115 [00:00<00:00, 176.09ba/s]\n",
      "Uploading the dataset shards: 100%|██████████| 3/3 [01:00<00:00, 20.20s/it]\n",
      "Map: 100%|██████████| 7354/7354 [00:01<00:00, 5649.92 examples/s]s]\n",
      "Creating parquet from Arrow format: 100%|██████████| 74/74 [00:00<00:00, 181.72ba/s]\n",
      "Uploading the dataset shards: 100%|██████████| 1/1 [00:09<00:00,  9.13s/it]\n",
      "Map: 100%|██████████| 7317/7317 [00:01<00:00, 7191.50 examples/s]s]\n",
      "Creating parquet from Arrow format: 100%|██████████| 74/74 [00:00<00:00, 180.59ba/s]\n",
      "Uploading the dataset shards: 100%|██████████| 1/1 [00:14<00:00, 14.28s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset successfully created and pushed to Hugging Face: https://huggingface.co/datasets/FLIP-Challenge\n"
     ]
    }
   ],
   "source": [
    "print(f\"Pushing dataset to Hugging Face as {dataset_name}...\")\n",
    "push_dataset_to_hf(dataset, dataset_name, token)\n",
    "\n",
    "print(f\"Dataset successfully created and pushed to Hugging Face: https://huggingface.co/datasets/{dataset_name}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "912c647f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
